\section{Test Methodology}
\subsection{Overview}
% The testing is mainly performed by the test team and the testing cross functional team. In the testing cross functional team, we have a analyst and software quality manager which will help verification with requirements in higher level testing. The testing is done iterative which mean that all testing is done in each sprint. This will help us to continuously improve both our application and our testing through each iteration.\\

The testing is mainly performed by the test team and the testing cross-functional team. In the testing cross-functional team, we have an analyst and software quality manager which will help verification with requirements in higher-level testing. The testing is done iterative which means that all testing is done in each sprint. This will help us to continuously improve both our application and our testing through each iteration. \\

\noindent Main workflow for testing new merge requests:\\
%  \emph{The developer posts a merge request in GitLab, in the merge request the developer must specify which feature that have been added and what they believe is important to test. The automated tests in the pipeline is performed. All merge requests needs two people to accept it, a company member and a tester. A company member performs a peer review of the code according to the set up rules. If the code follows company standards, the company member can accept the merge request. A tester assigns themselves to the merge request and removes the other testers if they are already assigned to the merge request by default. The predefined Selenium IDE tests are performed in the browser. Then the tester will perform additional exploratory testing on the specific feature that the merge request is related to. This is done both in desktop and in iPad mode. If a fault is found then the tester will post a comment on the merge request which specifies in which mode(iPad or desktop) the bug is found and exactly specify how to replicate the problem is so that the developer easily can find the problem by themselves. The developer who posted the merge is responsible for fixing the comment and reply to the comment saying that the problem is fixed. Then the tester will perform the testing procedure describe above again. If no problem is found then the tester can approve the merge request.}\\
  \emph{The The developer posts a merge request in GitLab; in the merge request, the developer must specify which feature has been added and what they believe is essential to test. The automated tests in the pipeline are performed. All merge requests need two people to accept them, a company member and a tester. A company member performs a peer review of the code according to the setup rules. If the code follows company standards, the company member can accept the merge request. A tester assigns themselves to the merge request and removes the other testers if they are already assigned to the merge request by default. The predefined Selenium IDE tests are performed in the browser. Then the tester will perform additional exploratory testing on the specific feature that the merge request is related to. This is done both on desktop and in iPad mode. If a fault is found, the tester will post a comment on the merge request which specifies in which mode(iPad or desktop) the bug is found and precisely specify how to replicate the problem so that the developer can easily find the problem by themselves. The developer who posted the merge is responsible for fixing the comment and replies to the comment, saying that the problem is fixed. Then the tester will perform the testing procedure described above again. If no problem is found, the tester can approve the merge request.}\\  
  
%  \noindent The testing of merge request will occur as soon as possible when a merge request have been posted in git. A merge request must be tested in 24 hours. If the testing team not have enough time to test then they will ask for help from the line manager (Daniel Ma).
 \noindent The testing of merge requests will occur as soon as possible when a merge request has been posted in git. A merge request must be tested in 24 hours. If the testing team does not have enough time to test, they will ask for help from the line manager (Daniel Ma).
\subsection{Test Levels}
In this chapter, the different test levels are presented and on which level we will test the application and why we don't test certain levels. The different testing levels are unit testing, module testing, system testing, and acceptance testing. This chapter will further explain the specifics of the different tests and why they are done.

%\subsubsection{Unit testing \& Module testing}
%In this project unit testing and module testing will not be performed. This will not be performed because the developed units are simple and few. Therefore, unit testing which take a lot of time because each unit needs to be tested without integration with the rest of the system. So, because we have limited time and the application only contains simple units we have chosen to not perform unit testing. \\

%\noindent Module testing is not performed because it is very time consuming because testing the integration between units we have to make drivers or stubs depending on approach. With the same reasoning as above we believe that our developed units and the integration between them are simple and therefore we do not perform testing on this level. A major reason we don't perform these test are that this project have limited time so we have to priorities which test we want to do. Therefore, we have chosen to perform test on system level because we believe that those test will result in the most efficient testing. 

\subsubsection{System testing}
% In this project the main testing will be performed on system level. We think this is the most important level of testing for this project since it is focused on the front-end and the users experience. In this chapter the different types of system level tests that are going to be performed are presented. 
The leading testing will be performed on the system level in this project. We think this is the most critical level of testing for this project since it is focused on the front-end and the user's experience. In this chapter, the different types of system-level tests that are going to be performed are presented.
\newline

\noindent \textbf{Pipeline}\\
% When a merge request is posted the gitlab pipeline is automatically activated. In the pipeline, a karma test is performed. This test will check if all the developed components are part of the application. This is a smoke test and is used to ensure that no functions are unused in the application. To check if the test is accepted, enter the specific job that was created for the merge request. Check at the bottom of the pipeline terminal and see if all test was successful. If all test were successful then the automated test in the pipeline can be seen as successful. The smoke test is performed to see that the build is stable and not causing any anomalies.
When a merge request is posted, the GitLab pipeline is automatically activated. In the pipeline, a karma test is performed. This test will check if all the developed components are part of the application. This is a smoke test and is used to ensure that no functions are unused in the application. To check if the test is accepted, enter the specific job created for the merge request. Check at the bottom of the pipeline terminal and see if all test was successful. If all tests were successful, then the automated test in the pipeline can be seen as successful. The smoke test is performed to see that the build is stable and not causing any anomalies.
\\ 
Features and automated tests that are not implemented before the end of the project will be noted and handed over to customer after project completion.  

\noindent\textbf{Functional testing}\\
% To test and verify our requirements, the test team will produce a list with an explanation how the requirement is tested. Several requirements that are linked together will be tested together in a test case. This is done to make sure that the application meets the requirements that are made by the analyst team. New test cases are added at the end of each sprint to test the added requirements that have been developed during the sprint. To help build test cases, Selenium IDE is used. When creating Selenium IDE it is important to cause events in different order because the user can click on things in different order. All requirements will have a explaining text of how they are going to be verified and which test cases they are part of. This information will be found in the document: \textit{Testing\_TestCases}  \\
To test and verify our requirements, the test team will produce a list explaining how the requirement is tested. Several linked requirements will be tested together in a test case. This is done to make sure that the application meets the requirements that are made by the analyst team. New test cases are added at the end of each sprint to test the added requirements that have been developed during the sprint. To help build test cases, Selenium IDE is used. When creating Selenium IDE, it is important to cause events in a different order because the user can click on things in a different order. All requirements will have an explanation text of how they are going to be verified and which test cases they are part of. This information will be found in the document: \textit{Testing\_TestCases}  \\

\noindent\textbf{Non-functional requirements}\\
% Each non-functional requirement will be tested separately and how it is tested will be documented in the excel file \textit{Testing\_TestCases}.The testing procedures we will mainly use is code review which will be performed by checking a requirement and then checking if it is implemented by looking in the code. Another way we will verify requirements if it is possible is to simple check on our application if it is fulfilled. For example, NUC-016 - Server calls shall be through open API:s for example based on REST/HTTPS, we will check if the server calls are through either REST or HTTPS in the code. All non functional requirements will be tested separately and marked if they are passed or not in the test case file. \\
Each non-functional requirement will be tested separately, and how it is tested will be documented in the excel file \textit{Testing\_TestCases}. The testing procedure we will mainly use is code review which will be performed by checking a requirement and then checking if it is implemented by looking in the code. Another way we will verify requirements, if possible, is to simply check on our application if it is fulfilled. For example, NUC-016 - Server calls shall be through open API:s; for example, based on REST/HTTPS, we will check if the server calls are through either REST or HTTPS in the code. All non-functional requirements will be tested separately and marked if they are passed or not in the test case file. \\

\noindent \textbf{Regression testing}\\
% Regression testing is going to be done to make sure that new functionality doesn't affect the old. When can achieve this by always running all of our selenium test cases for each merge request. In Selenium we set up test cases which tests a couple of requirements and these are preformed at each merge request. Selenium is used to help reduce the workload for the tester. In a event-driven environment the user can cause events in different orders which means that there are a lot of different test cases. Therefore, the test team will set up test cases in Selenium IDE that can be done for every merge request.\\
Regression testing is going to be done to make sure that new functionality does not affect the old. When can we achieve this by always running all of our selenium test cases for each merge request? In Selenium, we set up test cases that test a couple of requirements, and these are performed at each merge request. Selenium is used to help reduce the workload for the tester. In an event-driven environment, the user can cause events in different orders which means that there are a lot of different test cases. Therefore, the test team will set up test cases in Selenium IDE that can be done for every merge request.\\

% \noindent The Selenium tests will be updated and added iterative through out the project. If new feature are implemented a selenium will be developed to test that feature. If a small change is made to the system then the selenium test can be updated if necessary. At the end of the project there will be a Selenium test for each part of the system, for example testing the main menu, notifications and graphs. When running the selenium test, it is important to pay close attention to see that the system is working correctly. If a problem is found it is reported to the developer of the code by making a comment in the merge request in the same way that is explained under "the work flow for testing new merge requests". New Selenium tests will continuously be added when there is a new module that is added to the application. \\
\noindent The Selenium tests will be updated and added iterative throughout the project. If new features are implemented, a selenium will be developed to test that feature. If a small change is made to the system then the selenium test can be updated if necessary. At the end of the project, there will be a Selenium test for each part of the system, for example, testing the main menu, notifications, and graphs. When running the selenium test, paying close attention to see that the system is working correctly is vital. If a problem is found, it is reported to the developer of the code by commenting on the merge request in the same way that is explained under "the workflow for testing new merge requests". New Selenium tests will continuously be added when a new module is added to the application. \\
\subsubsection{Usability testing}
%  Usability testing is a also a big part of the project since the customer has expressed that the usability of the product is important. Since we are working iterative these tests are going to be performed at the end of each iteration when a new version of the product is released. 
Usability testing is also a big part of the project since the customer has expressed that the product's usability is essential. Since we are working on iterative, these tests are going to be performed at the end of each iteration when a new version of the product is released. 
\newline

% \noindent The usability test will be performed via teams and will be conducted with one tester and one customer. The meeting will be recorded so that the tester can go through the recording afterwards and write down the customers response in a usability report. The customer will be provided with different tasks and they are instructed to think aloud according to the CTA protocol. The tasks are created to test the whole application and will be revised before each usability test after the end of each iteration. This is done because new feature will be developed over time and should be reflected in the usability tests. A template will be created with the tasks and some initial questions about the customer. To get a quantitative result and measure the usability of the application a SUS-test will also be performed at the end the usability test. The test will be performed by giving the user tasks that shall be performed in the system. They will then answer a questionnaire with 10 questions and 5 answers to each question that range from 1 (strongly disagree) to 5 (strongly agree). The max score is 100. This test can be used to see how we are improving with regards to the usability by comparing the score in the different releases. The test is going to be performed by different users each time so that they do not get familiar with the application and base their score upon how the application has changed. The tasks can be found in the output file in teams. Because we are constantly developing new features the tasks are going to be changed between the usability tests. This is a source of error that needs to be taken in consideration when reviewing the SUS score. This is because each usability test from the user will be different with regards to the task they perform.
\noindent The usability test will be performed via teams and conducted with one tester and one customer. The meeting will be recorded so that the tester can go through the recording afterward and write down the customer's response in a usability report. The customer will be provided with different tasks, and they are instructed to think aloud according to the CTA protocol. The tasks are created to test the whole application and will be revised before each usability test after the end of each iteration. This is done because new features will be developed over time and should be reflected in the usability tests. A template will be created with the tasks and some initial questions about the customer. A SUS-test will also be performed at the end of the usability test to get a quantitative result and measure the usability of the application. The test will be performed by giving the user tasks that shall be performed in the system. They will then answer a questionnaire with 10 questions and 5 answers to each question ranging from 1 (strongly disagree) to 5 (strongly agree). The max score is 100. This test can be used to see how we are improving with regard to usability by comparing the score in the different releases. The test is going to be performed by different users each time so that they do not get familiar with the application and base their score on how the application has changed. The tasks can be found in the output file in teams. Because we are constantly developing new features, the tasks are going to be changed between the usability tests. This is a source of error that needs to be taken into consideration when reviewing the SUS score. This is because each usability test from the user will be different with regards to the task they perform.\
\newline

% \noindent The test will be conducted with the customer after each iteration to gather their response. Because the test is done after each iteration this enables the developer to get feedback of the not-yet-complete application by the customer. This will hopefully make the application tailored for the customer. The developed application will be used by people with different professions and technical experience. Therefore, it is important to conduct these usability tests with different level of technical experience, age and profession to get a complete picture of the true needs of the customer. Unfortunately the customer can not provide people with different profession due to specific professions that do not have time to spare. Therefore, some profession-specific feedback can be lost. Nonetheless, the doctors, nurses and assistant nurses works closely together and can give some insight on what other profession need.
\noindent After each iteration, the test will be conducted with the customer to gather their response. Because the test is done after each iteration, the developer can get feedback on the not-yet-complete application from the customer. This will hopefully make the application tailored for the customer. The developed application will be used by people with different professions and technical experience. Therefore, it is essential to conduct these usability tests with different levels of technical experience, age, and profession to get a complete picture of the customer's actual needs. Unfortunately, the customer can not provide people with different professions due to specific professions that do not have time to spare. Therefore, some profession-specific feedback can be lost. Nonetheless, the doctors, nurses, and assistant nurses work closely together and give some insight into what other professions need.
\newline

\noindent Dates for test with customer:\newline
\textbf{After iteration 1}\\
Meeting with customer, IT-staff and nurses participated: 11/10 - 21\newline
\textbf{After iteration 2}\\
Three different tests are going to be performed with different people:\newline
Usability test 1 with nurse: 11/11 - 21\newline
Usability test 2 with nurse: 12/11 - 21\newline
Usability test 3: 15/11 - 21\newline
\textbf{After iteration 3}\\
Purpose date for usability test: 22/11 - 21\newline
\textbf{After iteration 4, Final product}\\
Purposed date for usability testing: 3/12\newline

% \noindent Each test with the customer will result in a test report. The test report shall contain if the customer could complete each task and their feedback for each task. The test report will follow a standardized template that the test team will create. The different test reports for each test occasion will be summarized to a single test report for that iteration. This report will be distributed to the UX-team. This test report will be the foundation of changes in UX-design for the subsequent sprint. The UX-team will create prototypes so that the developers can easily understand our ambitions. This will help us continuously improve our application from the response from the customer.\newline
\noindent Each test with the customer will result in a test report. The test report shall contain whether the customer could complete each task and their feedback. The test report will follow a standardized template that the test team will create. The different test reports for each test occasion will be summarized to a single test report for that iteration. This report will be distributed to the UX team. This test report will be the foundation of changes in UX design for the subsequent sprint. The UX team will create prototypes so that the developers can easily understand our ambitions. This will help us continuously improve our application from the response from the customer.\newline

\noindent \textbf{UX/design-verification}\\
% Because UX and presentation of data is an important part of this project, we have a specific procedure for this verification. Two members from the UX-team will work together when reviewing the the UX and design of the application. By looking through the closed issues on GitLab, requirements to be tested can be found since every issue is linked to a requirement by the requirement code (e.g RC-002-001). The component will be reviewed, both on a web browser setting and in an iPad setting. The review will carry out by comparing the developed feature to the prototype and if they match, the requirement is passed and the issue is marked by a "passed review" label on Gitlab. If any faults are discovered, the review is failed and marked by a "failed review" label on GitLab. Additionally, a comment describing why the review failed is published and the issue is reopened to allow the assigned developer to fix the issues. The UX-designers will follow the prototype when reviewing the application. If needed, the UX-team will clarify the prototype on Figma to assist the developer. This is going to be performed for every iteration to make sure that passed requirements remain correct and that failed reviews eventually becomes passed as well. \\ 
Because UX and presentation of data is an essential part of this project, we have a specific procedure for this verification. Two members from the UX team will work together when reviewing the UX and design of the application. By looking through the closed issues on GitLab, requirements to be tested can be found since every issue is linked to a requirement by the requirement code (e.g., RC-002-001). The component will be reviewed, both on a web browser setting and in an iPad setting. The review will compare the developed feature to the prototype, and if they match, the requirement is passed, and the issue is marked by a "passed review" label on GitLab. If any faults are discovered, the review is failed and marked by a "failed review" label on GitLab. Additionally, a comment describing why the review failed is published, and the issue is reopened to allow the assigned developer to fix the issues. The UX designers will follow the prototype when reviewing the application. If needed, the UX team will clarify the prototype on Figma to assist the developer. This will be performed for every iteration to ensure that passed requirements remain correct and that failed reviews eventually become passed. \\ \\


\noindent\textbf{Traceability}\\
% To keep track and trace which components have been accepted throughout the process the UX-team will be using a work sheet, found in output folder on teams, where every requirement is present. The sheet consists of the requirement, a date of the latest review, who it is reviewed by and if the requirement has passed the review. It will also include a link to the relevant issue in git with all the relevant info of who developed, tested and peer-reviewed the code.\\
To track and trace which components have been accepted throughout the process, the UX team will use a worksheet found in the output folder on teams, where every requirement is present. The sheet consists of the requirement, the latest review date, whom it is reviewed by, and if the requirement has passed the review. It will also include a link to the relevant issue in git with all the relevant info of who developed, tested, and peer-reviewed the code.\\

% \noindent To make sure that every requirement is going to be tested and verified the test will go through the file with all the test cases. Because every requirement is mentioned it is easy to see if and how it is tested. It will also include a link to the relevant issue in git where information of who created, tested and peer-reviewed the code.\\  
\noindent To make sure that every requirement is going to be tested and verified, the test will go through the file with all the test cases. Because every requirement is mentioned, it is easy to see if and how it is tested. It will also include a link to the relevant issue in git where information of who created, tested, and peer-reviewed the code.\\  

\noindent\textbf{Quality}\\
% To ensure quality throughout the testing procedure no tester is going to test their own code. This is to minimize bias when evaluating the quality of the code. To identify the maximum number of bugs each test case must be completed by the tester before a merge request is approved. This strengthens the regression testing of the testing procedure as each function of the application must function as intended when new functionality is added. \\
To ensure quality throughout the testing procedure, no tester is going to test their own code. This is to minimize bias when evaluating the quality of the code. To identify the maximum number of bugs, the tester must complete each test case before a merge request is approved. This strengthens the regression testing of the testing procedure as each application function must function as intended when new functionality is added. \\

% \noindent The SUS-test conducted at the end of usability testing helps verify that the application is improving in quality between iterations. If the average SUS-score from usability testing is lower than that of the previous iteration things are not improving and processes and requirements need reevaluating before work is started for the coming iteration. The number of comments and suggestions of change on specific parts of the application from the user is also noted for each usability test. This ensures that already implemented parts of the application are becoming more accepted by the user.\\
\noindent The SUS-test conducted at the end of usability testing helps verify that the application is improving in quality between iterations. If the average SUS score from usability testing is lower than that of the previous iteration, things are not improving, and processes and requirements need reevaluating before work is started for the coming iteration. The number of comments and suggestions of change on specific parts of the application from the user is also noted for each usability test. This ensures that already implemented parts of the application are becoming more accepted by the user.\\

\subsubsection{Acceptance testing}
To validate the requirements, the test team will perform acceptance testing. These test are performed with the customer to validate that they accept the product. The acceptance testing is the final phase of testing, and this test is going to be performed by the intended users of the system. Black box testing will be used, and the test will be performed manually. This test will be done on the 3rd of December and will have the outcome of either a pass or a fail. A fail means that the customer does not approve the final product. A second acceptance test will then be performed on the 8th of December to see if any changes made during the last day result in the customer accepting the product. \newline

\noindent \textbf{Entry Criteria}\\
% When we conduct the acceptance testing, several criteria must be fulfilled. If we not manage to fulfill these criteria before the date of acceptance testing, we will have to postpone the test. If this event will occur we will perform an additional usability test with the customer instead and perform the acceptance test on the 8th of December instead. A list with the entry criteria is presented below:
When we conduct the acceptance testing, several criteria must be fulfilled. If we not manage to fulfill these criteria before the date of acceptance testing, we will have to postpone the test. If this event occur ,we will perform an additional usability test with the customer instead and perform the acceptance test on the 8th of December instead. A list with the entry criteria is presented below:
\begin{enumerate}
    \item All features should be completed
    % \item No major bugs should be found on the application. If small bugs are found, a list should be available for the customer with these bugs.
    \item No significant bugs should be found on the application. If minor bugs are found, a list should be available for the customer with these bugs.
    \item All system testing should be finished.
\end{enumerate}

\noindent \textbf{Exit Criteria}\\
These criterias need to be fulfilled to consider that the application can be launched.
\begin{enumerate}
    \item All tasks in the acceptance testing should be done and passed.
    \item The customer shall formally accept our application during the acceptance testing meeting.
\end{enumerate}

% \noindent The test team will create a template for the acceptance test with real life scenarios that the customer will perform on the application. The real life scenarios will be created based of the user stories and will be connected to several requirements. The customer will also give the chance to roam the application freely. If the customer can complete the task and they can accept the design, the task is defined as accepted. If all tasks are accepted then the application is defined as accepted. The test will be done in the same environment that the application is going to be used in. This means that the application will be deployed and the users can access it on their own computers/tablets from their workplace. A report from the acceptance test with the task and the outcome of the test will be created and added to the testing documents.
%
\noindent The test team will create a template for the acceptance test with real-life scenarios that the customer will perform on the application. The real-life scenarios will be created based on the user stories and connected to several requirements. The customer will also give the chance to roam the application freely. If the customer can complete the task and accept the design, the task is defined as accepted. If all tasks are accepted, then the application is defined as accepted. The test will be done in the same environment that the application is going to be used in. This means that the application will be deployed, and the users can access it on their computers/tablets from their workplace. A report from the acceptance test with the task and the outcome of the test will be created and added to the testing documents.
\newline


\noindent3-Dec- Final acceptance testing\newline
\noindent8-Dec- Ev. Final acceptance testing\newline

\subsection{Bug Triage}
% Bugs found during the testing procedure must be solved or have the underlying code generating the bug removed. This is due to all testing (except acceptance testing) taking place when a merge request is created. If a bug is found, the merge request is not approved by the tester. The merge request must then be edited to solve the bug or it is not to be merged at all. Bugs found outside of the testing procedure (e.g by non-testers) shall be reported to the testing team which will investigate the bug, create a Gitlab issue explaining what the bug is, steps to reproduce it, suspected underlying issue and deadline for when it must be solved. This is usually set to the next iteration deadline as known bugs shall not be present during acceptance testing with the customer. The Gitlab issue is assigned to the author of the code. If a bug is found during testing procedure but doesn't relate to the specific merge request. It will follow the same procedure as describe above minus the reporting to test team because the test team found the bug. 
Bugs found during the testing procedure must be solved or have the underlying code generating the bug removed. This is due to all testing (except acceptance testing) taking place when a merge request is created. If a bug is found, the tester does not approve the merge request. The merge request must then be edited to solve the bug or not to be merged at all. Bugs found outside of the testing procedure (e.g., by non-testers) shall be reported to the testing team which will investigate the bug, create a GitLab issue explaining what the bug is, steps to reproduce it, suspected underlying issue, and deadline for when it must be solved. This is usually set to the next iteration deadline as known bugs shall not be present during acceptance testing with the customer. The GitLab issue is assigned to the author of the code. Suppose a bug is found during the testing procedure but does not relate to the specific merge request. It will follow the same procedure described above minus reporting to the test team because the test team found the bug. \newline

\noindent Bugs found but not resolved within a week of code stop will be reported in a post project document delivered to the customer. Each of the bugs will have a description, suspected underlying issue and steps to replicate the bug.

\subsection{Suspension Criteria and Resumption Requirements}
Due to testers only working on testing and the small, limited number of functions in the application, the suspension criteria is only a complete system failure rendering the application non-functional.% Testers only working on testing means that continued runs after a program is not working correctly is reasonable in our time schedule. This is supported by the fact of a small, limited number of functions making each test short. This means that the only resumption criteria is when a non-functional program is made functional and thereby resuming testing.    
\subsubsection{Exam period, 19/10-21 $\xrightarrow{}$ 30/10-21}
% During the exam period neither developers nor testers will have time to perform testing or programming because they will study for their exams. Because all company participants have this period at the same time this will not create a bottleneck, only a temporary stop in the continuous delivery.
During the exam period, neither developers nor testers will have time to perform testing or programming because they will study for their exams. Because all company participants have this period simultaneously, this will not create a bottleneck, only a temporary stop in the continuous delivery.
\subsection{Test Completeness}
% A test is considered complete when it passes the Gitlab pipeline, the selenium IDE-tests and exploratory testing for each merge. In addition it also has to pass the UX-review. Not passing one of these tests means the merge request will not be approved. 
A test is considered complete when it passes the GitLab pipeline, the selenium IDE-tests, and exploratory testing for each merge. In addition, it also has to pass the UX review. Not passing one of these tests means the merge request will not be approved.

\subsection{Continuous Integration}
% Because we used continuous integration in this project we are going to have a working application at the end of each iteration. The working application will be used to conduct usability testing with the customer as previously described. The feedback we receive at the usability test will be summarized to a test report that will be distributed to the UX-team. The UX-team will create new and update our old prototypes based on the test rapport so that the developer easily understands how our requirements shall be implemented. If needed the test report will also be distributed to the analyst team if some requirements needs to be revised based on customer feedback. The process with testing and then forwarding information to the UX-team that will update our prototypes will occur for every iteration which will incrementally enhance our application with feedback from the customer.
Because we used continuous integration in this project, we are going to have a working application at the end of each iteration. The working application will be used to conduct usability testing with the customer as previously described. The feedback we receive at the usability test will be summarized in a test report that will be distributed to the UX team. The UX team will create new and update our old prototypes based on the test rapport so that the developer quickly understands how our requirements shall be implemented. If needed, the test report will also be distributed to the analyst team if some requirements need to be revised based on customer feedback. The process with testing and then forwarding information to the UX team that will update our prototypes will occur for every iteration which will incrementally enhance our application with feedback from the customer.