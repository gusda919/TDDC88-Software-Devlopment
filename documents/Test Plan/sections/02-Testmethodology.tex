\section{Test Methodology}
\subsection{Overview}
The testing is mainly performed by the test team and the testing cross functional team. In the testing cross functional team, we have a analyst and software quality manager which will help validation and verification in higher level testing. The testing is done iterative which mean that all testing is done in each sprint. This will help us to continuously improve both our application and our testing through each iteration.\\

\noindent The work flow for testing new merge requests:\\
 \emph{The developer posts a merge request in GitLab, in the merge request the developer must specify which feature that have been added and what they believe is important to test. The automated tests in the pipeline is performed. All merge requests needs two people to accept it, a company member and a tester. A company member performs a peer review according to the set up rules to check that the code is good enough. If the code follows company standards, the company member can accept the merge request. A tester assigns themselves to the merge request and removes the other testers if they are already assigned to the merge request by default. The predefined Selenium IDE tests are performed in the browser. Then the tester will perform additional exploratory testing on the specific feature that the merge request is related to. This is done both in desktop and in iPad mode. If a fault is found then the tester will post a comment on the merge request which specifies in which mode(iPad or desktop) the bug is found and exactly specify what the problem is so that the developer easily can find and replicate the problem by themselves. The developer who posted the merge is responsible for fixing the comment before they can post a new merge request and the same procedure will be performed again. If no problem is found then the tester can approve the merge request.}\\
 
 \noindent The testing of merge request will occur as soon as possible when a merge request have been posted in git. A merge request must be tested in 24 hours. If the testing team not have enough time to test then they will ask for help from the line manager (Daniel Ma).

\subsection{Test Levels}
In this chapter the different test levels are presented and on which level we will test the application and why we don't test certain levels. The different testing levels are unit testing, module testing, system testing and acceptance testing. This chapter will further explain the specifics of the different tests and why they are done.

\subsubsection{Unit testing \& Module testing}
In this project unit testing and module testing will not be performed. This will not be performed because the developed units are simple and few. Therefore, unit testing which take a lot of time because each unit needs to be tested without integration with the rest of the system. So, because we have limited time and the application only contains simple units we have chosen to not perform unit testing. \\

\noindent Module testing is not performed because it is very time consuming because testing the integration between units we have to make drivers or stubs depending on approach. With the same reasoning as above we believe that our developed units and the integration between them are simple and therefore we do not perform testing on this level. A major reason we don't perform these test are that this project have limited time so we have to priorities which test we want to do. Therefore, we have chosen to perform test on system level because we believe that those test will result in the most efficient testing. 

\subsubsection{System testing}
In this project the part of testing where the most time and resources are spent is the System testing. We think this is the most important level of testing for this project since it is focused on the front end and the users experience. In this chapter the different types of system tests that are going to be performed in is presented. \newline

\noindent \textbf{Pipeline}\\
When a merge request is purposed the gitlab pipeline is automatically activated. In the pipeline, a karma test is performed. This test will check if all the developed components are part of the application. This level of smoke testing is used to ensure that no functions are unused in the application. To check if the test is accepted, enter the specific job that was created for the merge request. Check at the bottom of the pipeline terminal and see if all test was successful. If all test were successful then the automated test in the pipeline can be seen as successful. The smoke test is performed to see that the build is stable and not causing any anomalies.\\ 

\noindent\textbf{Functional testing}\\
To test and verify our requirements, the test team will produce test cases that test several requirements at the same time. This is done to make sure that the application meets the requirements that are made by the analyst team. New test cases are added at the end of each sprint to test the added requirements that have been developed during the sprint. To help build test cases, Selenium IDE is used. When creating Selenium IDE it is important to cause events in different order because the user can click on things in different order. The test cases that are written comes from each of the requirements. The test team goes through the list of requirements and makes a test case for each requirement. Some test cases will verify more than one requirement. This is done to make sure that every requirement is met so that nothing is missed to be implemented or is implemented in the wrong way. If a requirement is not implemented or fails the test it is reported to the team leader of the developers.  \\

\noindent\textbf{Non-functional requirements}\\
To test non-functional requirements a code review will be performed for the most part. The code review will be performed by checking a requirement and then checking if it is implemented. For example, NUC-016 - Server calls shall be through open API:s for example based on REST/HTTPS, we will check if the server calls are through either REST or HTTPS. All non functional requirements will be tested separately and marked if they are done or not based on the code review.\\

\noindent \textbf{Regression testing}\\
Regression testing is done to make sure that new functionality doesn't affect the old. When can achieve this by always running all of our test cases for each merge request. In Selenium we set up test cases which tests a couple of requirements and these are preformed at each merge request. Additional test cases are also performed during the merge request phase. Selenium is used to help reduce the workload for the tester. In a event-driven environment the user can cause events in different orders which mean that there are a lot of different test cases. Therefore, the test team sets up test cases in Selenium IDE that can be done for every merge request.\\

\noindent The Selenium tests are the same each time to ensure that the new functionality that has been added is working correctly and does not affect the code in some way. There is Selenium tests that are added for each part of the system, for example testing the main menu, notifications and graphs. These tests will like said before be run every time even though if the new code that is being tested is not part of that module. If a Selenium test fail the tester will look at what is causing the problem by testing that part of the system manually. The problem is then reported to the developer of the code by making a comment in the merge request in the same way that is explained under "the work flow for testing new merge requests". New Selenium tests will continuously be added when there is a new module that is added to the application. \\

\subsubsection{Usability testing}
 Usability testing is a also a big part of the project since the customer has expressed that the usability of the product is important. Since we are working iterative these tests are going to be performed at the end of each iteration when a new version of the product is released. \newline

\noindent The usability test will be performed with the concurrent think aloud method. To get a quantitative result and measure the usability of the application a SUS-test will also be part of the usability test. The test will be performed by giving the user tasks that shall be performed in the system. They will then answer a questionnaire with 10 questions and 5 answers to each question that range from 1 (strongly disagree) to 5 (strongly agree). The max score is 100. This test can be used to see how we are improving with regards to the usability by comparing the score in the different releases. The test is going to be performed by different users each time so that they do not get familiar with the application and the score increases in that way. The concurrent think aloud questions and SUS-test will be
added as a appendix.\newline

\noindent The test will be conducted with the customer after each iteration to gather their response. Because the test is done after each iteration this enables the developer to get feedback of the not-yet-complete application by the customer. This will hopefully make the application tailored for the customer. The developed application will be used by people with different professions and technical experience. Therefore, it is important to conduct these usability tests with different level of technical experience, age and profession to get a complete picture of the true needs of the customer. Unfortunately the customer can not provide people with different profession due to specific profession does not have the time to spare. Therefore, some perspectives can be lost nonetheless the doctors, nurses and assistant nurses works close together and can give some insight on what other profession need. \newline

\noindent Dates for test with customer:\newline
\textbf{After iteration 1}\\
Meeting with customer, IT-staff and nurses participated: 11/10 - 21\newline
\textbf{After iteration 2}\\
Three different tests are going to be performed with different people:\newline
Usability test 1 with nurse: 11/11 - 21\newline
Usability test 2 with nurse: 12/11 - 21\newline
Usability test 3: 15/11 - 21\newline
\textbf{After iteration 3}\\
Purpose date for usability test: 22/11 - 21\newline
\textbf{After iteration 4, Final product}\\
Purposed date for acceptance testing: 3/12\newline

\noindent After iteration 3 and 4 there is also going to be multiple tests with different people like in iteration 2 but we have not confirmed dates or with who yet. The tests from each iteration is then going to be compared to each other to see how we have improved or if there is a specific area that has got a lower score than before to know which areas has to be improved to get a better score next time. After iteration 1 the application was working but there were to few features to perform worth-while CTA and SUS testing. Therefore, after iteration 1, the application was shown to the customer along with the prototype from the tollgate meeting. The meetings purpose was to give the customer an opportunity elaborate on their specific needs for different parts of the website, both functional and design related. From iteration 2 and forward the customer usability testing will all follow the same structure. The customer will be given a certain amount of tasks that they will perform on the application while following CTA-protocol. Afterwards, they will do a SUS-test. The tasks that the customer will be given can differ between the iterations because more features will be developed over time and thus, we want to include these feature in the test. \\

\noindent Each test with the customer will result in a test report. The different test reports for each test occasion will be summarized to a single test report that will be distributed to the whole company. This test report will be the foundation of changes in requirements and UX-design for the subsequent sprint. The test report after iteration 4 will show if the usability requirements are accepted correctly from a customer point of view.\newline

\noindent \textbf{UX-verification}\\
Two members from the UX-team work together when reviewing user interface requirements. By looking through the closed issues on GitLab, requirements to be tested can be found since every issue is linked to a requirement by the requirement code (e.g RC-002-001). The component is then reviewed, both on a web browser setting and in an iPad setting. The developed feature is compared to the prototype and if they match, the requirement is passed and the issue is marked by a "passed review" label on Gitlab. If any faults are discovered, the review is failed and marked by a "failed review" label on GitLab. Additionally, a comment describing why the review failed is published and the issue is reopened to allow the assigned developer to fix the issues. The UX-designers will follow the prototype when reviewing the application. If needed, the UX-team enhance the prototype on Figma to assist the developer. This is done every iteration to make sure that passed requirements remain correct and that failed reviews eventually becomes passed as well. \\ 

\noindent\textbf{Traceability}\\
To keep track and trace which components have been accepted throughout the process the UX-team will be using a work sheet, found in output folder on teams, where every requirement is present. The sheet consists of the requirement, a date of the latest review, who it is reviewed by and if the requirement has passed the review. It will also include a link to the relevant issue in git with all the relevant info of who developed, tested and peer-reviewed the code.\\

\noindent\textbf{Quality}\\
To ensure quality throughout the testing procedure no tester is to have written part of the code they are testing. This is to minimize bias when evaluating the quality of the code. To identify the maximum number of bugs each test case must be completed by the tester before a merge request is approved. This strengthens the regression testing of the testing procedure as each function of the application must function as intended when new functionality is added. \\

\noindent The SUS-test conducted at the end of usability testing helps verify that the application is improving in quality between iterations. If the average SUS-score from usability testing is lower than that of the previous iteration things are not improving and processes and requirements need reevaluating before work is started for the coming iteration. The number of comments or suggestions of change on specific parts of the application from a user for each usability test is also noted to ensure that already implemented parts of the application are becoming more accepted by the user.\\

\subsubsection{Acceptance testing}
To validate the requirements the test team will perform acceptance testing. These test are performed with the customer to validate that they accept the product. The acceptance testing is the final phase of testing, this test is performed by the intended users of the system. Black box testing will be used and the test will be performed manually. This test will be done on the 3rd of December and will have the outcome of either a pass or a fail. A fail means that the customer does not approve the final product. A second acceptance test will then be performed the 8th of December. \newline

\noindent \textbf{Entry Criteria}\\
When we conduct the acceptance testing, several criteria must be fulfilled. A list of them is presented below:
\begin{enumerate}
    \item All features should be completed
    \item No major bugs should be found on the application. If small bugs are found, a list should be available for the customer with these bugs.
    \item All system testing should be done.
\end{enumerate}

\noindent \textbf{Exit Criteria}\\
These criterias need to be fulfilled to consider that the application can be launched.
\begin{enumerate}
    \item All tasks in the acceptance testing should be done and passed
    \item The customer shall formally accept our application during the acceptance testing meeting.
\end{enumerate}

\noindent Test scenarios will be created and each of the scenarios will be connected to one or more requirements. From these scenarios we will create more specific test cases that cover all of the scenarios. The users will perform the test cases. The test will be done in the same environment that the application is going to be used in. This means that the application will be deployed and the users can access it on their own computers/tablets. A rapport for the acceptance test with the task and the outcome of the test will be created and added to the testing documents.
\newline


\noindent3-Dec- Final acceptance testing\newline
\noindent8-Dec- Ev. Final acceptance testing\newline

\begin{comment}
\subsection{Continuous Integration}
It is the practice of integrating changes from different developers in the team into a mainline as early as possible, in best cases several times a day. This makes sure the code individual developers work on doesn’t divert too much. When you combine the process with automated testing, continuous integration can enable your code to be dependable.(From internet)
\end{comment}
\subsection{Bug Triage}
Bugs found during the testing procedure must be solved or have the underlying code generating the bug removed. This is due to all testing (except acceptance testing) taking place when a merge request is created. If a bug is found, the merge request is not approved by the tester. The merge request must then be edited to solve the bug or it is not to be merged at all. Bugs found outside of the testing procedure (e.g by non-testers) shall be reported to the testing team which will investigate the bug, create a Gitlab issue explaining what the bug is, steps to reproduce it, suspected underlying issue and deadline for when it must be solved. This is usually set to the next iteration deadline as known bugs shall not be present during acceptance testing with the customer. The Gitlab issue is assigned to the author of the code. 

\subsection{Suspension Criteria and Resumption Requirements}
Due to testers only working on testing and the small, limited number of functions in the application the suspension criteria is only a complete system failure rendering the application non-functional. Testers only working on testing means that continued runs after a program is not working correctly is reasonable in our time schedule. This is supported by the fact of a small, limited number of functions making each test short. This means that the only resumption criteria is when a non-functional program is made functional and thereby resuming testing.    
\subsubsection{Exam period, 19/10-21 $\xrightarrow{}$ 30/10-21}
During the exam period neither developers nor testers will have time to perform testing or programming because they will study for their exams. Because all company participants have this period at the same time this will not create a bottleneck, only a temporary stop in the continuous delivery.
\subsection{Test Completeness}
A test is considered complete when it passes the Gitlab pipeline, the selenium IDE-tests and exploratory testing for each merge. Not passing one of these tests means the merge request will not be approved.

\subsection{Continuous Integration}
Because we used continuous integration in this project we are going to have a working application at the end of each iteration. The working application will be used to conduct usability testing with the customer as previously described. The feedback we receive at the usability test will be summarized to a test rapport that will be distributed to the UX-team. The UX-team will create new and update our old prototypes based on the test rapport so that the developer easily understands how our requirements shall be implemented. If needed the test rapport will also be distributed to the analyst team if some requirements needs to be revised based on customer feedback. The process with testing and then forwarding information to the UX-team that will update our prototypes will occur for every iteration which will incrementally enhance our application with feedback from the customer.